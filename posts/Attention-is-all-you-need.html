<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mi primera vez</title>
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <main>
    <header>
	<nav>
            <a href="../index.html">[jrey.ink]</a>
            <a href="../fotos.html">[fotiños]</a>
            <a href="../libros.html">[libros]</a>
            <a href="../posts.html">[posts]</a>
            <a href="../lists.html">[listas]</a>
        </nav>

    </header>
    <div class="divider"></div>
    <article>
        <h1> Attention Is All You Need</h1>
        <p class="subtitle">Junio 13, 2024</p>
	<h2>Disclaimer</h2>
	<p>
	Antes de nada, sobre el temita que más da que hablar con respecto a la IA, que si nos va a quitar el trabajo y todo eso, blablablá. Como gran aficionado del estar tumbao, que me quiten el trabajo me parece de putísima madre. Y al que le guste trabajar, no tiene de que preocuparse, no creo que nadio le vaya a impedir hacerlo gratis. No sé, subiendo una roca a una montaña y dejándola caer, por ejemplo.
	</p>
	<p>
	Ahora bien, si por alguna razón trabajar gratis no les parece lo suficientemente edificante, que no les dignifica o que no le ven propósito y todas estas mierdas, siempre podrán inventarse algún sistema de incentivos en el que reciban a cambio premios o medallitas, o, aún mejor, dinero fiduciario que no está respaldado en nada ni valga casi nada, para que, cuanto más ganen, más palmaditas en la espalda les den y puedan sentirse realizados de alguna forma.
	</p>
	<p>	
	Ya cada quien.
	</p>
       
    <div class="divider"></div>
    <br>
    <br>

    <p>
    En 2017, el mundo de la inteligencia artificial experimentó un punto de inflexión con la publicación del paper "Attention Is All You Need" por Vaswani et al. Este trabajo revolucionó el procesamiento del lenguaje natural (NLP) al introducir el modelo Transformer, una arquitectura basada exclusivamente en mecanismos de atención, dejando atrás las recurrentes (RNN) y convolucionales (CNN) que dominaban el estado del arte en ese momento.
    </p>

  <p>
La clave del éxito del Transformer radica en su capacidad para procesar secuencias de datos de forma paralela, gracias al mecanismo de Self-Attention, que evalúa la importancia relativa entre cada palabra (o token) de una entrada. Esto permitió manejar dependencias a largo plazo con una eficiencia sin precedentes y establecer un nuevo estándar en tareas como traducción automática, clasificación de texto y modelado de lenguaje.
  </p>

  <p>
Desde entonces, los modelos basados en Transformers han transformado el panorama de la inteligencia artificial. Tecnologías como GPT y BERT han dominado el procesamiento del lenguaje, mientras que extensiones multimodales como DALL·E o CLIP han llevado estas capacidades a imágenes y otras modalidades. Este progreso, impulsado por la escalabilidad y el acceso a grandes volúmenes de datos, ha dado lugar a sistemas generativos capaces de no solo comprender, sino también crear en niveles que antes parecían imposibles.
  </p>

     </article>
    </main>
</body>
</html>
